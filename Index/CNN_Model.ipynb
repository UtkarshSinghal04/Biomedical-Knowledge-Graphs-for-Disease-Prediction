{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T10:28:15.934427Z",
     "iopub.status.busy": "2025-05-14T10:28:15.934112Z",
     "iopub.status.idle": "2025-05-14T12:02:02.580338Z",
     "shell.execute_reply": "2025-05-14T12:02:02.579244Z",
     "shell.execute_reply.started": "2025-05-14T10:28:15.934401Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded pretrained embeddings\n",
      "Loaded 1957551 total triples for filtering\n",
      "Train triples: 2170877\n",
      "Validation triples: 271768\n",
      "Test triples: 271573\n",
      "Setting embedding dimension to 128 from pretrained embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 16960/16960 [20:26<00:00, 13.83it/s, Loss=0.0590]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 500/500 [02:34<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - MRR: 0.0636, Hits@1: 0.0170, Hits@3: 0.0630, Hits@10: 0.1390\n",
      "Saved new best model with MRR: 0.0636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 16960/16960 [20:29<00:00, 13.80it/s, Loss=0.0556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating epoch 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 500/500 [02:31<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - MRR: 0.0867, Hits@1: 0.0270, Hits@3: 0.0850, Hits@10: 0.1990\n",
      "Saved new best model with MRR: 0.0867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 16960/16960 [20:29<00:00, 13.80it/s, Loss=0.0556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating epoch 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 500/500 [02:34<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - MRR: 0.0874, Hits@1: 0.0290, Hits@3: 0.0720, Hits@10: 0.2080\n",
      "Saved new best model with MRR: 0.0874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 16960/16960 [20:29<00:00, 13.80it/s, Loss=0.0556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating epoch 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 500/500 [02:34<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - MRR: 0.0827, Hits@1: 0.0330, Hits@3: 0.0770, Hits@10: 0.1980\n",
      "No improvement for 1 validations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:   6%|▋         | 1082/16960 [01:18<19:18, 13.70it/s, Loss=0.0556]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/3065627577.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_31/3065627577.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;31m# Train with MRR-based early stopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m     train_model(\n\u001b[0m\u001b[1;32m    375\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_triples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_triples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mentity_to_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31/3065627577.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_triples, all_triples, entity_to_id, optimizer, device, epochs, patience)\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m             \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{total_loss/(batch_idx+1):.4f}\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "class BiomedicalKGDataset(Dataset):\n",
    "    def __init__(self, triple_file, entity_to_id, relation_to_id, negative_samples=50):\n",
    "        self.entity_to_id = entity_to_id\n",
    "        self.relation_to_id = relation_to_id\n",
    "        self.num_entities = len(entity_to_id)\n",
    "        self.negative_samples = negative_samples\n",
    "        \n",
    "        # Load and filter triples\n",
    "        self.triples = []\n",
    "        with open(triple_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                h, r, t = line.strip().split('\\t')\n",
    "                if h in entity_to_id and r in relation_to_id and t in entity_to_id:\n",
    "                    self.triples.append((entity_to_id[h], relation_to_id[r], entity_to_id[t]))\n",
    "        \n",
    "        # Create filtered set for negative sampling\n",
    "        self.positive_set = set(map(tuple, self.triples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        h, r, t = self.triples[idx]\n",
    "        pos_triple = torch.tensor([h, r, t], dtype=torch.long)\n",
    "        \n",
    "        # Generate negative samples\n",
    "        neg_triples = []\n",
    "        for _ in range(self.negative_samples):\n",
    "            # Corrupt head\n",
    "            neg_h = random.randint(0, self.num_entities-1)\n",
    "            while (neg_h, r, t) in self.positive_set:\n",
    "                neg_h = random.randint(0, self.num_entities-1)\n",
    "            neg_triples.append([neg_h, r, t])\n",
    "            \n",
    "            # Corrupt tail\n",
    "            neg_t = random.randint(0, self.num_entities-1)\n",
    "            while (h, r, neg_t) in self.positive_set:\n",
    "                neg_t = random.randint(0, self.num_entities-1)\n",
    "            neg_triples.append([h, r, neg_t])\n",
    "        \n",
    "        neg_triples = torch.tensor(neg_triples, dtype=torch.long)\n",
    "        labels = torch.cat([torch.ones(1), torch.zeros(len(neg_triples))], dim=0)\n",
    "        return pos_triple, neg_triples, labels\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pos_batch = torch.stack([item[0] for item in batch])\n",
    "    neg_batch = torch.cat([item[1] for item in batch])\n",
    "    labels = torch.cat([item[2] for item in batch])\n",
    "    return torch.cat([pos_batch, neg_batch]), labels\n",
    "\n",
    "class SimplifiedCNNModel(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, embed_dim=200, dropout=0.3, l2_lambda=1e-5):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.l2_lambda = l2_lambda\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.entity_embed = nn.Embedding(num_entities, embed_dim)\n",
    "        self.rel_embed = nn.Embedding(num_relations, embed_dim)\n",
    "        \n",
    "        # Simplified CNN architecture to prevent overfitting\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(embed_dim, 128, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.AdaptiveMaxPool1d(1)\n",
    "        )\n",
    "        \n",
    "        # Simple fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LayerNorm(32),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_normal_(self.entity_embed.weight)\n",
    "        nn.init.xavier_normal_(self.rel_embed.weight)\n",
    "\n",
    "    def forward(self, triples):\n",
    "        h = self.entity_embed(triples[:, 0])\n",
    "        r = self.rel_embed(triples[:, 1])\n",
    "        t = self.entity_embed(triples[:, 2])\n",
    "        \n",
    "        # Stack embeddings for convolution [batch, embed_dim, 3]\n",
    "        stacked = torch.stack([h, r, t], dim=2)\n",
    "        \n",
    "        # Apply CNN\n",
    "        conv_out = self.conv(stacked).squeeze(-1)\n",
    "        \n",
    "        # Apply FC layers\n",
    "        return self.fc(conv_out).squeeze()\n",
    "\n",
    "    def load_pretrained_embeddings(self, entity_emb, rel_emb):\n",
    "        \"\"\"Load pretrained embeddings if available\"\"\"\n",
    "        if entity_emb is not None:\n",
    "            # Ensure dimensions match\n",
    "            if self.entity_embed.weight.shape == entity_emb.shape:\n",
    "                self.entity_embed.weight.data.copy_(torch.from_numpy(entity_emb))\n",
    "            else:\n",
    "                print(f\"Entity embedding shapes don't match. Model: {self.entity_embed.weight.shape}, Pretrained: {entity_emb.shape}\")\n",
    "        \n",
    "        if rel_emb is not None:\n",
    "            # Ensure dimensions match\n",
    "            if self.rel_embed.weight.shape == rel_emb.shape:\n",
    "                self.rel_embed.weight.data.copy_(torch.from_numpy(rel_emb))\n",
    "            else:\n",
    "                print(f\"Relation embedding shapes don't match. Model: {self.rel_embed.weight.shape}, Pretrained: {rel_emb.shape}\")\n",
    "    \n",
    "    def l2_regularization(self):\n",
    "        return self.l2_lambda * (self.entity_embed.weight.norm(2)**2 + \n",
    "                                self.rel_embed.weight.norm(2)**2)\n",
    "\n",
    "def compute_loss(scores, labels, model):\n",
    "    bce_loss = nn.BCEWithLogitsLoss()(scores, labels)\n",
    "    return bce_loss + model.l2_regularization()\n",
    "\n",
    "def compute_metrics(model, test_triples, all_triples, entity_to_id, device, batch_size=512):\n",
    "    \"\"\"\n",
    "    Compute ranking metrics like MRR and Hits@k with filtered evaluation.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    ranks = []\n",
    "    \n",
    "    # Create filtered dictionary\n",
    "    filter_dict = defaultdict(set)\n",
    "    for h, r, t in all_triples:\n",
    "        filter_dict[(h, r)].add(t)  # For tail prediction\n",
    "        filter_dict[(t, r)].add(h)  # For head prediction (with inverse relation)\n",
    "    \n",
    "    entity_ids = torch.arange(len(entity_to_id)).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for h, r, t in tqdm(test_triples, desc=\"Evaluating\"):\n",
    "            # Head prediction\n",
    "            head_ranks = []\n",
    "            \n",
    "            for start in range(0, len(entity_to_id), batch_size):\n",
    "                end = min(start + batch_size, len(entity_to_id))\n",
    "                current_entities = entity_ids[start:end]\n",
    "                \n",
    "                # Create batch for head prediction\n",
    "                hr_batch = torch.zeros((len(current_entities), 3), dtype=torch.long, device=device)\n",
    "                hr_batch[:, 0] = current_entities\n",
    "                hr_batch[:, 1] = r\n",
    "                hr_batch[:, 2] = t\n",
    "                \n",
    "                # Get scores\n",
    "                scores = model(hr_batch).cpu().numpy()\n",
    "                \n",
    "                # Filter out other true triples\n",
    "                for j, e in enumerate(current_entities.cpu().numpy()):\n",
    "                    if e != h and e in filter_dict.get((t, r), set()):\n",
    "                        scores[j] = -np.inf\n",
    "                \n",
    "                # If true head is in this batch\n",
    "                if start <= h < end:\n",
    "                    h_idx = h - start\n",
    "                    h_score = scores[h_idx]\n",
    "                    h_rank = 1 + np.sum(scores > h_score)\n",
    "                    head_ranks.append(h_rank)\n",
    "            \n",
    "            if head_ranks:\n",
    "                ranks.append(min(head_ranks))\n",
    "            \n",
    "            # Tail prediction\n",
    "            tail_ranks = []\n",
    "            \n",
    "            for start in range(0, len(entity_to_id), batch_size):\n",
    "                end = min(start + batch_size, len(entity_to_id))\n",
    "                current_entities = entity_ids[start:end]\n",
    "                \n",
    "                # Create batch for tail prediction\n",
    "                tr_batch = torch.zeros((len(current_entities), 3), dtype=torch.long, device=device)\n",
    "                tr_batch[:, 0] = h\n",
    "                tr_batch[:, 1] = r\n",
    "                tr_batch[:, 2] = current_entities\n",
    "                \n",
    "                # Get scores\n",
    "                scores = model(tr_batch).cpu().numpy()\n",
    "                \n",
    "                # Filter out other true triples\n",
    "                for j, e in enumerate(current_entities.cpu().numpy()):\n",
    "                    if e != t and e in filter_dict.get((h, r), set()):\n",
    "                        scores[j] = -np.inf\n",
    "                \n",
    "                # If true tail is in this batch\n",
    "                if start <= t < end:\n",
    "                    t_idx = t - start\n",
    "                    t_score = scores[t_idx]\n",
    "                    t_rank = 1 + np.sum(scores > t_score)\n",
    "                    tail_ranks.append(t_rank)\n",
    "            \n",
    "            if tail_ranks:\n",
    "                ranks.append(min(tail_ranks))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    ranks_array = np.array(ranks)\n",
    "    mrr = np.mean(1.0 / ranks_array)\n",
    "    \n",
    "    hits = {}\n",
    "    for k in [1, 3, 10]:\n",
    "        hits[k] = np.mean(ranks_array <= k)\n",
    "    \n",
    "    return {'MRR': mrr, **{f'Hits@{k}': hits[k] for k in hits}}\n",
    "\n",
    "def train_model(model, train_loader, val_triples, all_triples, entity_to_id, optimizer, device, epochs=100, patience=5):\n",
    "    \"\"\"\n",
    "    Train the model with early stopping based on validation MRR.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    best_mrr = 0.0\n",
    "    patience_counter = 0\n",
    "    validate_every = 1  # Validate every N epochs after the first few\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch_idx, (triples, labels) in enumerate(pbar):\n",
    "            triples = triples.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            scores = model(triples)\n",
    "            loss = compute_loss(scores, labels, model)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({'Loss': f\"{total_loss/(batch_idx+1):.4f}\"})\n",
    "        \n",
    "        # Validation using MRR\n",
    "        should_validate = (epoch + 1) % validate_every == 0\n",
    "        if should_validate:\n",
    "            print(f\"\\nValidating epoch {epoch+1}...\")\n",
    "            \n",
    "            # Sample validation triples to speed up validation\n",
    "            sampled_val_triples = random.sample(val_triples, min(len(val_triples), 500))\n",
    "            metrics = compute_metrics(model, sampled_val_triples, all_triples, entity_to_id, device)\n",
    "            \n",
    "            print(f\"Validation - MRR: {metrics['MRR']:.4f}, Hits@1: {metrics['Hits@1']:.4f}, \"\n",
    "                  f\"Hits@3: {metrics['Hits@3']:.4f}, Hits@10: {metrics['Hits@10']:.4f}\")\n",
    "            \n",
    "            val_mrr = metrics['MRR']\n",
    "            \n",
    "            if val_mrr > best_mrr:\n",
    "                best_mrr = val_mrr\n",
    "                patience_counter = 0\n",
    "                torch.save(model.state_dict(), 'best_model.pt')\n",
    "                print(f\"Saved new best model with MRR: {best_mrr:.4f}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                print(f\"No improvement for {patience_counter} validations\")\n",
    "                \n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load data\n",
    "    base_path = '/kaggle/input/embeddings-triplet-dataset'  # Adjust to your path\n",
    "    \n",
    "    with open(os.path.join(base_path, 'entity_to_id.json'), 'r') as f:\n",
    "        entity_to_id = json.load(f)\n",
    "    with open(os.path.join(base_path, 'relation_to_id.json'), 'r') as f:\n",
    "        relation_to_id = json.load(f)\n",
    "    \n",
    "    # Try to load pretrained embeddings if they exist\n",
    "    entity_emb = None\n",
    "    rel_emb = None\n",
    "    try:\n",
    "        entity_emb = np.load(os.path.join(base_path, 'entity_embeddings_continued.npy'))\n",
    "        rel_emb = np.load(os.path.join(base_path, 'relation_embeddings_continued.npy'))\n",
    "        print(\"Loaded pretrained embeddings\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Pretrained embeddings not found, will initialize randomly\")\n",
    "    \n",
    "    # Load all triples for filtered evaluation\n",
    "    all_triples = set()\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        with open(os.path.join(base_path, f'{split}.tsv')) as f:\n",
    "            for line in f:\n",
    "                h, r, t = line.strip().split('\\t')\n",
    "                if h in entity_to_id and r in relation_to_id and t in entity_to_id:\n",
    "                    all_triples.add((\n",
    "                        entity_to_id[h], relation_to_id[r], entity_to_id[t]\n",
    "                    ))\n",
    "    \n",
    "    print(f\"Loaded {len(all_triples)} total triples for filtering\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_set = BiomedicalKGDataset(\n",
    "        os.path.join(base_path, 'train.tsv'),\n",
    "        entity_to_id,\n",
    "        relation_to_id,\n",
    "        negative_samples=50  # Research-backed optimal value for biomedical KGs\n",
    "    )\n",
    "    \n",
    "    # Load validation/test triples\n",
    "    def load_triples(split):\n",
    "        triples = []\n",
    "        with open(os.path.join(base_path, f'{split}.tsv')) as f:\n",
    "            for line in f:\n",
    "                h, r, t = line.strip().split('\\t')\n",
    "                if h in entity_to_id and r in relation_to_id and t in entity_to_id:\n",
    "                    triples.append((\n",
    "                        entity_to_id[h], relation_to_id[r], entity_to_id[t]\n",
    "                    ))\n",
    "        return triples\n",
    "    \n",
    "    val_triples = load_triples('val')\n",
    "    test_triples = load_triples('test')\n",
    "    \n",
    "    print(f\"Train triples: {len(train_set)}\")\n",
    "    print(f\"Validation triples: {len(val_triples)}\")\n",
    "    print(f\"Test triples: {len(test_triples)}\")\n",
    "    \n",
    "    # Create data loader\n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size=128, shuffle=True, \n",
    "        collate_fn=collate_fn, num_workers=4, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Get embedding dimension from pretrained embeddings if available\n",
    "    embed_dim = 200\n",
    "    if entity_emb is not None:\n",
    "        embed_dim = entity_emb.shape[1]\n",
    "        print(f\"Setting embedding dimension to {embed_dim} from pretrained embeddings\")\n",
    "    \n",
    "    # Initialize model with simplified architecture\n",
    "    model = SimplifiedCNNModel(\n",
    "        num_entities=len(entity_to_id),\n",
    "        num_relations=len(relation_to_id),\n",
    "        embed_dim=embed_dim,\n",
    "        dropout=0.3,\n",
    "        l2_lambda=1e-5\n",
    "    )\n",
    "    \n",
    "    # Load pretrained embeddings if available\n",
    "    if entity_emb is not None and rel_emb is not None:\n",
    "        model.load_pretrained_embeddings(entity_emb, rel_emb)\n",
    "    \n",
    "    # Use AdamW optimizer with weight decay\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    \n",
    "    # Train with MRR-based early stopping\n",
    "    train_model(\n",
    "        model, train_loader, val_triples, all_triples, \n",
    "        entity_to_id, optimizer, device, epochs=10, patience=5\n",
    "    )\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"\\n=== Final Test Evaluation ===\")\n",
    "    model.load_state_dict(torch.load('best_model.pt', map_location=device))\n",
    "    test_metrics = compute_metrics(model, test_triples, all_triples, entity_to_id, device)\n",
    "    \n",
    "    print(f\"MRR: {test_metrics['MRR']:.4f}\")\n",
    "    print(f\"Hits@1: {test_metrics['Hits@1']:.4f}\")\n",
    "    print(f\"Hits@3: {test_metrics['Hits@3']:.4f}\")\n",
    "    print(f\"Hits@10: {test_metrics['Hits@10']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7414184,
     "sourceId": 11805725,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
